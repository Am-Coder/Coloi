{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2,preprocess_input\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from skimage import io, color,transform\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "for img in os.listdir('C:/Users/HP/coloi3/images/Train1'):\n",
    "    X.append(image.img_to_array(image.load_img('C:/Users/HP/coloi3/images/Train1/'+img,target_size=(256,256))))\n",
    "    \n",
    "X = np.array(X,dtype=float)\n",
    "Xtrain = 1.0/255*X[:int(0.97*len(X))]\n",
    "Xtest = 1.0/255*X[int(0.97*len(X)):]\n",
    "\n",
    "\n",
    "    \n",
    "  #Globlal Feature extractor\n",
    "inception = InceptionResNetV2(weights='imagenet',include_top=True)\n",
    "# inception.load_weights('inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "# print(inception.layers)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "embed_inp = Input(shape=(1000,))\n",
    "# inception.graph = tf.get_default_graph() #Makes inception model graph == default graph\n",
    "\n",
    "#Encoder Network via functional API\n",
    "enc_inp = Input(shape=(256,256,1))\n",
    "encode = Conv2D(64,(3,3),strides=2,activation='relu',padding='same',name='d')(enc_inp)\n",
    "encode = Conv2D(128,(3,3),strides=1,activation='relu',padding='same',name='e')(encode)\n",
    "encode = Conv2D(128,(3,3),strides=2,activation='relu',padding='same',name='f')(encode)\n",
    "encode = Conv2D(256,(3,3),strides=1,activation='relu',padding='same',name='g')(encode)\n",
    "encode = Conv2D(256,(3,3),strides=2,activation='relu',padding='same',name='h')(encode)\n",
    "encode = Conv2D(512,(3,3),strides=1,activation='relu',padding='same',name='i')(encode)\n",
    "encode = Conv2D(512,(3,3),strides=1,activation='relu',padding='same',name='j')(encode)\n",
    "encode = Conv2D(256,(3,3),strides=1,activation='relu',padding='same',name='k')(encode)\n",
    "\n",
    "#fusion layer\n",
    "fuse = RepeatVector(32*32)(embed_inp)\n",
    "fuse = Reshape([32,32,1000])(fuse)\n",
    "fuse = concatenate([encode,fuse],axis=3)\n",
    "fuse = Conv2D(256,(1,1),activation='relu',padding='same',name='l')(fuse)\n",
    "\n",
    "\n",
    "#Decoder for final colorization\n",
    "decode = Conv2D(128,(3,3),activation='relu',padding='same',name='m')(fuse)\n",
    "decode = UpSampling2D(size=(2,2),name='A')(decode)\n",
    "decode = Conv2D(64,(3,3),activation='relu',padding='same',name='n')(decode)\n",
    "decode = UpSampling2D(size=(2,2),name='B')(decode)\n",
    "decode = Conv2D(32,(3,3),activation='relu',padding='same',name='o')(decode)\n",
    "decode = Conv2D(16,(3,3),activation='relu',padding='same',name='p')(decode)\n",
    "decode = Conv2D(2,(3,3),activation='tanh',padding='same',name='q')(decode)\n",
    "decode = UpSampling2D(size=(2,2),name='C')(decode)\n",
    "\n",
    "#Finally,model\n",
    "\n",
    "model = Model(inputs=[enc_inp,embed_inp],outputs=decode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding for global feature extractor(I mean InceptioResnetV2)\n",
    "def emb_for_gfe(rgb):\n",
    "    rgb_resized = []\n",
    "#     print(rgb.shape)\n",
    "    for i in rgb:\n",
    "        i = transform.resize(i,(299,299,3),mode='constant')\n",
    "#         i = i.reshape((299,299,3))\n",
    "        rgb_resized.append(i)\n",
    "    rgb_resized = np.array(rgb_resized)\n",
    "    rgb_resized = preprocess_input(rgb_resized)\n",
    "   \n",
    "\n",
    "    with graph.as_default():\n",
    "        embed = inception.predict(rgb_resized)\n",
    "    return embed\n",
    "\n",
    "#Transorms\n",
    "datagen = image.ImageDataGenerator( shear_range=0.4,\n",
    "                                    zoom_range=0.4,\n",
    "                                    rotation_range=40,\n",
    "                                    horizontal_flip=True)\n",
    "b_size = 3\n",
    "\n",
    "\n",
    "def generator(b_size):\n",
    "    for batch in datagen.flow(Xtrain,batch_size=b_size):\n",
    "        gray_rgb = color.gray2rgb(color.rgb2gray(batch))\n",
    "        embed = emb_for_gfe(gray_rgb)\n",
    "        lab = color.rgb2lab(batch)\n",
    "        x_batch = (lab[:,:,:,0])#Luminence\n",
    "        x_batch = x_batch.reshape(x_batch.shape + (1,))\n",
    "        y_batch = lab[:,:,:,1:]/128\n",
    "        yield([x_batch,embed],y_batch)\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 11s 11s/step - loss: 0.0171\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 1s 857ms/step - loss: 0.2852\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 1s 860ms/step - loss: 0.0074\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 1s 855ms/step - loss: 0.0278\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "0.0032526645809412003\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "with graph.as_default():\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit_generator(generator(b_size),epochs=4,steps_per_epoch=1)\n",
    "\n",
    "\n",
    "#test\n",
    "test_rgb = color.gray2rgb(color.rgb2gray(Xtest))\n",
    "embed_test = emb_for_gfe(test_rgb)\n",
    "lab = color.rgb2lab(Xtest)\n",
    "x_test = (lab[:,:,:,0])#Luminence\n",
    "x_test = x_test.reshape(x_test.shape + (1,))\n",
    "y_test = lab[:,:,:,1:]/128\n",
    "\n",
    "print(model.evaluate([x_test,embed_test], y_test,batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
